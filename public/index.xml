<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>favstats - personal blog on favstats - personal blog</title>
    <link>/</link>
    <description>Recent content in favstats - personal blog on favstats - personal blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Fabio Votta</copyright>
    <lastBuildDate>Mon, 25 Dec 2017 00:00:00 +0100</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Visualizing Temperature Rise in Stuttgart, Germany over Time</title>
      <link>/post/temperature_viz/</link>
      <pubDate>Sat, 01 Sep 2018 00:00:00 +0200</pubDate>
      
      <guid>/post/temperature_viz/</guid>
      <description>

&lt;p&gt;This is a quick use-case of gganimate to visualize the rise of average temperature in my home town, Stuttgart, Germany.&lt;/p&gt;

&lt;h2 id=&#34;get-packages&#34;&gt;Get Packages&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pacman::p_load(tidyverse, rvest, gganimate)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;get-and-save-data&#34;&gt;Get and Save Data&lt;/h2&gt;

&lt;p&gt;You can get the data from here: &lt;a href=&#34;https://icdc.cen.uni-hamburg.de/daten/atmosphere/dwd-station.html&#34; target=&#34;_blank&#34;&gt;https://icdc.cen.uni-hamburg.de/daten/atmosphere/dwd-station.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The German Weather Service (DWD) provides climate data for more than 70 observation stations from the measurement network in Germany. The stations provide scheduled, daily and monthly readings of temperatures, rainfall, sunshine duration, wind speed, humidity, barometric pressure and cloud cover, which are updated daily.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In this use-case, we are only interested in the temperature in &lt;em&gt;Stuttgart&lt;/em&gt;. The available data for the observation station in Stuttgart ranges from 1953 until today.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;url &amp;lt;- &amp;quot;http://icdc.cen.uni-hamburg.de/las/ProductServer.do?xml=%3C%3Fxml+version%3D%221.0%22%3F%3E%3ClasRequest+href%3D%22file%3Alas.xml%22%3E%3Clink+match%3D%22%2Flasdata%2Foperations%2Foperation%5B%40ID%3D%27DBExtractRowset%27%5D%22%3E%3C%2Flink%3E%3Cproperties%3E%3Cferret%3E%3Ccomponents%3E+grw_m+antdyn_m+greendyn_m+antsmb_m+ocn_m+greensmb_m+glac_m+gia_m%3C%2Fcomponents%3E%3Cview%3Exy%3C%2Fview%3E%3C%2Fferret%3E%3C%2Fproperties%3E%3Cargs%3E%3Cconstraint+id%3D%22undefined%22+type%3D%22text%22%3E%3Cv%3EStations_Kennziffer%3C%2Fv%3E%3Cv%3E%253D%3C%2Fv%3E%3Cv%3E10738%3C%2Fv%3E%3C%2Fconstraint%3E%3Clink+match%3D%22%2Flasdata%2Fdatasets%2Fdwd_data%2Fvariables%2Favg_temp_day%22%3E%3C%2Flink%3E%3Cregion%3E%3Crange+type%3D%22t%22+low%3D%2201-Jan-1876%22+high%3D%2227-Aug-2018%22%3E%3C%2Frange%3E%3Crange+type%3D%22x%22+low%3D%220%22+high%3D%2224%22%3E%3C%2Frange%3E%3Crange+type%3D%22y%22+low%3D%2240%22+high%3D%2259%22%3E%3C%2Frange%3E%3C%2Fregion%3E%3C%2Fargs%3E%3C%2FlasRequest%3E&amp;quot;

temp_stgt &amp;lt;- read_html(url) %&amp;gt;% 
  rvest::html_node(&amp;quot;table&amp;quot;) %&amp;gt;% html_table()

if(!dir.exists(&amp;quot;data&amp;quot;)) dir.create(&amp;quot;data&amp;quot;)

temp_stgt &amp;lt;- temp_stgt %&amp;gt;% 
  janitor::clean_names() %&amp;gt;% 
  rename(avg_temp_day = luftemperatur_avg) %&amp;gt;% 
  mutate(t = lubridate::ymd(t)) %&amp;gt;% 
  mutate(month = lubridate::month(t)) %&amp;gt;% 
  mutate(day = lubridate::yday(t)) %&amp;gt;% 
  arrange(t) %&amp;gt;% 
  mutate(year = lubridate::year(t)) %&amp;gt;% 
  mutate(time = year) %&amp;gt;% 
  group_by(year) %&amp;gt;% 
  mutate(avg_temp_year_year = mean(avg_temp_day)) %&amp;gt;% 
  ungroup()

save(temp_stgt, file = &amp;quot;data/temp_stgt.Rdata&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;plot-temperature-static&#34;&gt;Plot Temperature (Static)&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;load(&amp;quot;data/temp_stgt.Rdata&amp;quot;)

months &amp;lt;- c(&amp;quot;Jan&amp;quot;, &amp;quot;Feb&amp;quot;, &amp;quot;Mar&amp;quot;, &amp;quot;Apr&amp;quot;, &amp;quot;May&amp;quot;, &amp;quot;Jun&amp;quot;, &amp;quot;Jul&amp;quot;, &amp;quot;Aug&amp;quot;, &amp;quot;Sep&amp;quot;, &amp;quot;Oct&amp;quot;, &amp;quot;Nov&amp;quot;, &amp;quot;Dec&amp;quot;)

temp_stgt %&amp;gt;% 
  # filter(year %in% c(1960, 1970, 1980, 1990, 2000, 2010, 2018)) %&amp;gt;% 
  ggplot(aes(day, avg_temp_day, color = year, group = year)) +
  geom_point(size = .01, alpha = .1) +
  geom_line(size = .01, alpha = .1) +
  geom_smooth(se = F, size = .01, alpha = .2) +
  viridis::scale_color_viridis(&amp;quot;Year&amp;quot;, direction = -1, discrete = F,
                               breaks = c(1953, 1985, 2018),
                               labels = c(1953, 1985, 2018)) +
  ggthemes::theme_hc() +
  labs(title = &amp;quot;Average Daily Temperature in Stuttgart (1953 - 2018)&amp;quot;, 
       caption = &amp;quot;Data: Der Deutsche Wetterdienst (DWD)&amp;quot;,
       y = &amp;quot;Average Daily Temperature&amp;quot;, x = &amp;quot;&amp;quot;) +
  scale_x_continuous(breaks = seq(0, 365, length.out = 12), labels = months) +
  guides(colour = guide_colourbar(barwidth = 20, label.position = &amp;quot;bottom&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/favstats/temperature_viz/blob/master/images/avg_temp_year.png?raw=true&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/favstats/temperature_viz/blob/master/images/avg_temp_year.png?raw=true&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;if(!dir.exists(&amp;quot;images&amp;quot;)) dir.create(&amp;quot;images&amp;quot;)

ggsave(filename = &amp;quot;images/avg_temp_year.png&amp;quot;, height = 6, width = 10)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;animations&#34;&gt;Animations&lt;/h2&gt;

&lt;h3 id=&#34;average-temperature-over-time-by-day&#34;&gt;Average Temperature over Time by Day&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;temp_stgt_all &amp;lt;- temp_stgt %&amp;gt;% 
  select(-year)

p1 &amp;lt;- temp_stgt %&amp;gt;% 
  # filter(year %in% c(1960, 1970, 1980, 1990, 2000, 2010, 2018)) %&amp;gt;% 
  ggplot(aes(day, avg_temp_day, color = avg_temp_year, group = year)) +
  geom_smooth(data = temp_stgt_all, aes(day, avg_temp_day, group = time), 
              color = &amp;quot;grey&amp;quot;, se = F, size = .1, alpha = .1) +
  geom_point(size = .5, alpha = .7) +
  geom_line(size = .5, alpha = .7) +  
  viridis::scale_color_viridis(&amp;quot;Average Yearly Temperature&amp;quot;, direction = -1, discrete = F) +
  geom_smooth(se = F, size = 1, alpha = .7) +

  ggthemes::theme_hc()  +
  geom_text(aes(x = 340, y = 28, label = paste0(&amp;quot;Year = &amp;quot;, year)), size = 3, 
            hjust = 1, color = &amp;quot;black&amp;quot;) +
  geom_text(aes(x = 340, y = 26, 
                label = paste0(&amp;quot;Avg. Temperature = &amp;quot;, 
                           sprintf(&amp;quot;%.2f&amp;quot;, round(avg_temp_year, 2)), &amp;quot;°C&amp;quot;)), 
                                  size = 3, color = &amp;quot;black&amp;quot;, hjust = 1) +
  labs(title = &amp;quot;Average Daily Temperature in Stuttgart (1953 - 2018)&amp;quot;, 
       caption = &amp;quot;Data: Der Deutsche Wetterdienst (DWD)&amp;quot;,
       y = &amp;quot;Average Daily Temperature&amp;quot;, x = &amp;quot;&amp;quot;) +
  scale_x_continuous(breaks = seq(0, 365, length.out = 12), labels = months) +
  guides(colour = guide_colourbar(barwidth = 20, label.position = &amp;quot;bottom&amp;quot;)) +
  # Here comes the gganimate code
  transition_time(
    year
  ) +
  enter_fade() + 
  exit_fade() +
  ease_aes(&#39;linear&#39;)

magick::image_write(
  image = animate(p1,  width = 1000, height = 600, renderer = magick_renderer(), length = 45), 
  path = &amp;quot;images/daily_temp.gif&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/favstats/temperature_viz/blob/master/images/daily_temp.gif?raw=true&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/favstats/temperature_viz/blob/master/images/daily_temp.gif?raw=true&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;average-temperature-over-time-by-year&#34;&gt;Average Temperature over Time by Year&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;temp_stgt_allyear &amp;lt;- temp_stgt %&amp;gt;% 
  filter(year != 2018) %&amp;gt;%
  group_by(year) %&amp;gt;% 
  summarise(avg_temp_year = mean(avg_temp_year)) %&amp;gt;% 
  mutate(time = year) %&amp;gt;% 
  select(-year)

p2 &amp;lt;- temp_stgt %&amp;gt;% 
  filter(year != 2018) %&amp;gt;%
  group_by(year) %&amp;gt;% 
  summarise(avg_temp_year = mean(avg_temp_year)) %&amp;gt;% 
  ggplot(aes(year, avg_temp_year)) +
  geom_smooth(data = temp_stgt_allyear, aes(time, avg_temp_year), 
              color = &amp;quot;red&amp;quot;, se = F, size = .8) +
  geom_line(data = temp_stgt_allyear, aes(time, avg_temp_year), 
              color = &amp;quot;black&amp;quot;, size = .8, alpha = .6) +
  geom_point(data = temp_stgt_allyear, aes(time, avg_temp_year, color = avg_temp_year), size = 2.5) +
  geom_point(aes(color = avg_temp_year), size = 5) +
  ggthemes::theme_hc()  +
  geom_text(aes(x = 1953.5, y = 10.8, 
                label = paste0(&amp;quot;Year = &amp;quot;, year)), 
                size = 3, color = &amp;quot;black&amp;quot;,
                hjust = 0) +
  geom_text(aes(x = 1953.5, y = 10.7, 
                label = paste0(&amp;quot;Avg. Temperature = &amp;quot;, 
                           sprintf(&amp;quot;%.2f&amp;quot;, round(avg_temp_year, 2)), &amp;quot;°C&amp;quot;)), 
                                  size = 3, color = &amp;quot;black&amp;quot;, hjust = 0) +
  viridis::scale_color_viridis(&amp;quot;Average Yearly Temperature&amp;quot;, direction = -1, discrete = F)  +
  labs(title = &amp;quot;Average Yearly Temperature in Stuttgart (1953 - 2017)&amp;quot;, 
       caption = &amp;quot;Data: Der Deutsche Wetterdienst (DWD)&amp;quot;,
       y = &amp;quot;Average Yearly Temperature&amp;quot;, x = &amp;quot;&amp;quot;) +
  guides(colour = guide_colourbar(barwidth = 20, label.position = &amp;quot;bottom&amp;quot;)) +
  # Here comes the gganimate code
  transition_time(
    year
  ) +
  enter_fade() + 
  exit_fade() +
  ease_aes(&#39;linear&#39;) +
  ease_aes(&#39;linear&#39;)

magick::image_write(
  image = animate(p2,  width = 1000, height = 600, renderer = magick_renderer(), length = 45), 
  path = &amp;quot;images/yearly_temp.gif&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/favstats/temperature_viz/blob/master/images/yearly_temp.gif?raw=true&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/favstats/temperature_viz/blob/master/images/yearly_temp.gif?raw=true&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;average-temperature-over-time-by-year-1&#34;&gt;Average Temperature over Time by Year&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p3 &amp;lt;- temp_stgt %&amp;gt;% 
  filter(year %in% c(1953, 1960, 1990, 2017)) %&amp;gt;%
  ggplot(aes(day, avg_temp_day, group = year, color = year)) +
  geom_line() + 
  geom_segment(aes(xend = 365, yend = avg_temp_day), linetype = 2, colour = &#39;grey&#39;) + 
  geom_point(size = 2) + 
  geom_text(aes(x = 365, label = year), hjust = 0, size = 3, fontface = &amp;quot;bold&amp;quot;) + 
  coord_cartesian(clip = &#39;off&#39;) + 
  ggthemes::theme_hc()  +
  labs(title = &amp;quot;Average Daily Temperature in Stuttgart (1953, 1960, 1990, 2017)&amp;quot;, 
       caption = &amp;quot;Data: Der Deutsche Wetterdienst (DWD)&amp;quot;,
       y = &amp;quot;Average Daily Temperature&amp;quot;, x = &amp;quot;&amp;quot;) +
  scale_x_continuous(breaks = seq(0, 365, length.out = 12), labels = months) +
  viridis::scale_color_viridis(&amp;quot;Average Yearly Temperature&amp;quot;, direction = -1, discrete = F) +
  guides(colour = F) +
  theme(title = element_text(size = 15, face = &amp;quot;bold&amp;quot;), 
        axis.text.x = element_text(size = 14, face = &amp;quot;bold&amp;quot;), 
        axis.text.y = element_text(size = 10, face = &amp;quot;italic&amp;quot;)) +
  # Here comes the gganimate code
  transition_reveal(year, day) + 
  enter_fade() + 
  exit_fade() +
  ease_aes(&#39;linear&#39;) 

# create animation
p3 %&amp;gt;% animate(
  nframes = 500, fps = 15, width = 1000, height = 600, detail = 3
)

anim_save(&amp;quot;images/daily_temp_reveal.gif&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/favstats/temperature_viz/blob/master/images/daily_temp_reveal.gif?raw=true&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/favstats/temperature_viz/blob/master/images/daily_temp_reveal.gif?raw=true&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;R version 3.5.0 (2018-04-23)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows &amp;gt;= 8 x64 (build 9200)

Matrix products: default

locale:
[1] LC_COLLATE=German_Germany.1252  LC_CTYPE=German_Germany.1252   
[3] LC_MONETARY=German_Germany.1252 LC_NUMERIC=C                   
[5] LC_TIME=German_Germany.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] bindrcpp_0.2.2       rvest_0.3.2          xml2_1.2.0           forcats_0.3.0       
 [5] stringr_1.3.0        dplyr_0.7.6          purrr_0.2.5          readr_1.1.1         
 [9] tidyr_0.8.1          tibble_1.4.2         tidyverse_1.2.1      gganimate_0.9.9.9999
[13] ggplot2_3.0.0       

loaded via a namespace (and not attached):
 [1] viridis_0.5.1     httr_1.3.1        jsonlite_1.5      viridisLite_0.3.0
 [5] transformr_0.1.0  modelr_0.1.1      assertthat_0.2.0  cellranger_1.1.0 
 [9] progress_1.2.0    pillar_1.2.1      lattice_0.20-35   glue_1.3.0       
[13] digest_0.6.16     colorspace_1.4-0  plyr_1.8.4        psych_1.8.3.3    
[17] lpSolve_5.6.13    pkgconfig_2.0.1   devtools_1.13.5   broom_0.4.4      
[21] gifski_0.8.3      haven_1.1.2       magick_1.9        patchwork_0.0.1  
[25] scales_1.0.0      tweenr_0.1.5.9999 git2r_0.23.0      farver_1.0       
[29] pacman_0.4.6      withr_2.1.2       lazyeval_0.2.1    cli_1.0.0        
[33] mnormt_1.5-5      magrittr_1.5      crayon_1.3.4      readxl_1.1.0     
[37] memoise_1.1.0     nlme_3.1-137      foreign_0.8-70    class_7.3-14     
[41] ggthemes_4.0.0    tools_3.5.0       prettyunits_1.0.2 hms_0.4.2        
[45] munsell_0.5.0     compiler_3.5.0    e1071_1.6-8       rlang_0.2.2      
[49] classInt_0.2-3    units_0.6-0       grid_3.5.0        rstudioapi_0.7   
[53] labeling_0.3      gtable_0.2.0      DBI_1.0.0         curl_3.2         
[57] reshape2_1.4.3    R6_2.2.2          gridExtra_2.3     lubridate_1.7.4  
[61] knitr_1.20        bindr_0.1.1       stringi_1.1.7     parallel_3.5.0   
[65] Rcpp_0.12.18      sf_0.6-3          png_0.1-7         spData_0.2.8.3   
[69] tidyselect_0.2.4 
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>How does Collinearity Influence Linear Regressions?</title>
      <link>/post/multicol_sim/</link>
      <pubDate>Tue, 28 Aug 2018 00:00:00 +0200</pubDate>
      
      <guid>/post/multicol_sim/</guid>
      <description>

&lt;p&gt;This is a short simulation study trying to figure out the impact of collinearity on linear regressions.&lt;/p&gt;

&lt;h2 id=&#34;packages&#34;&gt;Packages&lt;/h2&gt;

&lt;p&gt;Load the necessary packages&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# install pacman once if not avaible on your machine
# install.packages(&amp;quot;pacman&amp;quot;)

pacman::p_load(arm, purrr, MASS, broom, ggthemes, tidyverse, ecodist, viridis, gridExtra, grid, lm.beta, tidyr, ggrepel)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;simulation-function&#34;&gt;Simulation Function&lt;/h2&gt;

&lt;p&gt;First, I write a little function to simulate collinearity.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;generate_multi &amp;lt;- function(n, cor_seq){
  set.seed(2017)
  x &amp;lt;- runif(n, 1, 10)
  models &amp;lt;- list()
  std.models &amp;lt;- list()
  for (jj in seq_along(cor_seq)) {
    ## generate correlated variable x2
    dat &amp;lt;- data.frame(corgen(x = x, r = cor_seq[jj],  epsilon = 0))
    colnames(dat) &amp;lt;- c(&amp;quot;x1&amp;quot;, &amp;quot;x2&amp;quot;)
    ## generate y variable
    dat$y &amp;lt;- 0.5 * dat$x1 + 0.5 * dat$x2 + rnorm(n, sd = 10)
    ## modelling and tidy dataframe
    models[[jj]] &amp;lt;- tidy(lm(y ~ x1 + x2, data = dat))
    ## get standardized betas
    std.models[[jj]] &amp;lt;- data.frame(lm.beta::coef.lm.beta((lm.beta::lm.beta(lm(y ~ x1 + x2, data = dat)))))
    colnames(std.models[[jj]]) &amp;lt;- c(&amp;quot;std.estimate&amp;quot;)  
    ## bind it together
    models[[jj]] &amp;lt;- std.models[[jj]] %&amp;gt;% 
                      bind_cols(models[[jj]]) 
    models[[jj]]$cors &amp;lt;- cor_seq[jj]
  }
  sim_dat &amp;lt;- bind_rows(models)
  sim_dat$col &amp;lt;- n
  return(sim_dat)
}

draw.data &amp;lt;- function(cor_seq = NULL, step_seq = NULL){
    sim.list &amp;lt;- list()
    for(jj in seq_along(step_seq)) {
      sim.list[[jj]] &amp;lt;- generate_multi(n = step_seq[jj], cor_seq)
      sim.list[[jj]]$n &amp;lt;- step_seq[jj]
      
      cat(paste0(&amp;quot;Batch: &amp;quot;, jj, &amp;quot;\t&amp;quot;))
      
    } 
    sim_data &amp;lt;- bind_rows(sim.list)
    return(sim_data)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;simulate-data&#34;&gt;Simulate Data&lt;/h2&gt;

&lt;p&gt;Draw data from function and save it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sim_data &amp;lt;- draw.data(cor_seq = seq(0,.99,0.01), step_seq = seq(50, 10000, by = 50))

if(!dir.exists(&amp;quot;data&amp;quot;)) dir.create(&amp;quot;data&amp;quot;)

save(sim_data, file = &amp;quot;data/sim_data.Rdata&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;visualizing-the-influence-of-collinearity&#34;&gt;Visualizing the Influence of Collinearity&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;load(&amp;quot;data/sim_data.Rdata&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, consider the following linear regression: y ~ x1 + x2.&lt;/p&gt;

&lt;p&gt;\begin{equation}
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon
\end{equation}&lt;/p&gt;

&lt;p&gt;For this simulation, I consistently increase the correlation between x1 and x2 (from 0 to .99) and the sample size (from 50 to 10.000) and estimate separate models for each of these combinations,&lt;/p&gt;

&lt;h3 id=&#34;standard-errors&#34;&gt;Standard Errors&lt;/h3&gt;

&lt;p&gt;First, I take a look at the impact that sample size and collinearity have on the standard error of x1. On the x-axis, you can see the increasing collinearity between x1 and x2 and the individual lines are colored in by sample size. There are a few things we can observe here: the standard error diminishes with greater sample size, correlations around .75 increase the standard error quite drastically and this effect is weaker the higher your sample size is.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;get_smooths &amp;lt;- function(smooth_dat, n_val, y) {
  smooth_dat &amp;lt;- filter(smooth_dat, n == n_val &amp;amp; term == &amp;quot;x1&amp;quot;)
  fm &amp;lt;- paste0(y,&amp;quot; ~ cors&amp;quot;)
  smooth_vals &amp;lt;- predict(loess(fm, smooth_dat), smooth_dat$cors) 
  
  smooth_dat %&amp;gt;% 
  mutate(smooth = smooth_vals) %&amp;gt;% 
  group_by(n) %&amp;gt;% 
  summarise(max_smooth = max(smooth),
            min_smooth = min(smooth)
            )
}

smooth_dat &amp;lt;- c(50, 100, 150, 200, 10000) %&amp;gt;% 
  map_df(~get_smooths(sim_data, n_val = .x, y = &amp;quot;std.error&amp;quot;)) %&amp;gt;% 
  mutate(n_lab = ifelse(n == 50, &amp;quot;Sample Size: 50&amp;quot;, n))

sim_data %&amp;gt;% 
  filter(term == &amp;quot;x1&amp;quot;) %&amp;gt;% 
  ggplot(aes(cors, std.error, colour = n, group = n)) + 
  geom_smooth(method = &amp;quot;loess&amp;quot;, se = F, size = 1, alpha = 0.5) +
  xlab(expression(&amp;quot;Pearson&#39;s&amp;quot;~r~correlation~between~x[1]~and~x[2])) + 
  ylab(expression(x[1]~Standard~Error)) + 
  theme_hc() + 
  scale_color_viridis(&amp;quot;Sample Size&amp;quot;, direction = -1,
       breaks = seq(1000, 10000, 3000),
       labels = seq(1000, 10000, 3000)) +
  ggtitle(&amp;quot;Sample Size and Collinearity Influence on Standard Error&amp;quot;) +
  geom_point(data = smooth_dat, aes(x = .99, y = max_smooth)) +
  geom_text_repel(data = smooth_dat, aes(x = .99, y = max_smooth, label = n_lab), 
                  nudge_y = 0.07, nudge_x = 0.03) +
  guides(colour = guide_colourbar(barwidth = 20, label.position = &amp;quot;bottom&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/favstats/multicol_sim/blob/master/images/std_static.png?raw=true&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggsave(filename = &amp;quot;images/std_static.png&amp;quot;, width = 10, height = 7)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;t-statistic-and-p-values&#34;&gt;T-Statistic and P-Values&lt;/h3&gt;

&lt;p&gt;Next, let&amp;rsquo;s take a look at the t-statistic and p-values. In line with what you would expect with greater standard errors, statistical significance also suffers from collinearity. Again, greater sample sizes seem to partly remedy this but the shift is clearly observable.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sim_data  %&amp;gt;% 
     filter(term == &amp;quot;x1&amp;quot;) %&amp;gt;% 
     ggplot(aes(cors, statistic, colour = n, group = n)) + 
     geom_smooth(method = &amp;quot;loess&amp;quot;, se = F, size = 1, alpha = 0.5) +
  xlab(expression(&amp;quot;Pearson&#39;s&amp;quot;~r~correlation~between~x[1]~and~x[2])) + 
  ylab(expression(x[1]~t-Statistic)) + 
  theme_hc() + 
  scale_color_viridis(&amp;quot;Sample Size&amp;quot;, direction = -1,
       breaks = seq(1000, 10000, 3000),
       labels = seq(1000, 10000, 3000)) +
  geom_hline(yintercept = 1.96, linetype = &amp;quot;dashed&amp;quot;, alpha = 0.9) +
  annotate(geom = &amp;quot;text&amp;quot;, x = 0, y = 2.3, label = &amp;quot;t = 1.96&amp;quot;) +
  ggtitle(&amp;quot;Sample Size and Collinearity Influence on t-Statistic&amp;quot;) +
  guides(colour = guide_colourbar(barwidth = 20, label.position = &amp;quot;bottom&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/favstats/multicol_sim/blob/master/images/t_static.png?raw=true&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggsave(filename = &amp;quot;images/t_static.png&amp;quot;, width = 10, height = 7)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sim_data  %&amp;gt;% 
     filter(term==&amp;quot;x1&amp;quot;) %&amp;gt;% 
     ggplot(aes(cors, p.value, colour = n, group = n)) + 
     geom_smooth(method = &amp;quot;loess&amp;quot;, se = F, size = 1, alpha = 0.5) +
  xlab(expression(&amp;quot;Pearson&#39;s&amp;quot;~r~correlation~between~x[1]~and~x[2])) + 
  ylab(expression(x[1]~p-value)) + 
  theme_hc() + 
  scale_color_viridis(&amp;quot;Sample Size&amp;quot;, direction = -1,
       breaks = seq(1000, 10000, 3000),
       labels = seq(1000, 10000, 3000)) +
  geom_hline(yintercept = 0.05, linetype = &amp;quot;dashed&amp;quot;, alpha = 0.9) +
  annotate(geom = &amp;quot;text&amp;quot;, x = 0, y = 0.08, label = &amp;quot;p = 0.05&amp;quot;) +
  ggtitle(&amp;quot;Sample Size and Collinearity Influence on p-values&amp;quot;) +
  guides(colour = guide_colourbar(barwidth = 20, label.position = &amp;quot;bottom&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/favstats/multicol_sim/blob/master/images/p_static.png?raw=true&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggsave(filename = &amp;quot;images/p_static.png&amp;quot;, width = 10, height = 7)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;b-coefficients&#34;&gt;B-Coefficients&lt;/h4&gt;

&lt;p&gt;This one I find most interesting. In addition to statistical signif. being affected, you can also see how the coefficients change with increasing collinearity. The most drastic impact is for small sample sizes (~1000) where a coefficient of .5 can even become negative.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sim_data  %&amp;gt;%
    filter(term==&amp;quot;x1&amp;quot;) %&amp;gt;%
    filter(n&amp;gt;200) %&amp;gt;%
    ggplot(aes(cors, estimate, colour = n, group = n)) +
    geom_hline(yintercept = 0.5, linetype = &amp;quot;dashed&amp;quot;, alpha = 0.9) +
    geom_line(alpha = 0.5) +
  xlab(expression(&amp;quot;Pearson&#39;s&amp;quot;~r~correlation~between~x[1]~and~x[2])) + 
  ylab(expression(x[1]~b-coefficient)) + 
  theme_hc() + 
  scale_color_viridis(&amp;quot;Sample Size&amp;quot;, direction = -1,
       breaks = seq(1000, 10000, 3000),
       labels = seq(1000, 10000, 3000)) +
  annotate(geom = &amp;quot;text&amp;quot;, x = 0, y = 0.08, label = &amp;quot;b = 0.5&amp;quot;) +
  ggtitle(&amp;quot;Sample Size and Collinearity Influence on b-coefficients&amp;quot;) +
  guides(colour = guide_colourbar(barwidth = 20, label.position = &amp;quot;bottom&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/favstats/multicol_sim/blob/master/images/b_static.png?raw=true&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggsave(filename = &amp;quot;images/b_static.png&amp;quot;, width = 10, height = 7)
&lt;/code&gt;&lt;/pre&gt;

&lt;h5 id=&#34;standardized&#34;&gt;Standardized&lt;/h5&gt;

&lt;p&gt;Same spiel, just this time with standardized coefficients.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sim_data  %&amp;gt;% 
     filter(term==&amp;quot;x1&amp;quot;) %&amp;gt;% 
     filter(n&amp;gt;200) %&amp;gt;%
     ggplot(aes(cors, std.estimate, colour = n, group = n)) + 
    geom_line(alpha = 0.5) +
  xlab(expression(&amp;quot;Pearson&#39;s&amp;quot;~r~correlation~between~x[1]~and~x[2])) + 
  ylab(expression(x[1]~b-coefficient)) + 
  theme_hc() + 
  scale_color_viridis(&amp;quot;Sample Size&amp;quot;, direction = -1,
       breaks = seq(1000, 10000, 3000),
       labels = seq(1000, 10000, 3000)) +
  ggtitle(&amp;quot;Sample Size and Collinearity Influence on standardized b-coefficients&amp;quot;) +
  guides(colour = guide_colourbar(barwidth = 20, label.position = &amp;quot;bottom&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/favstats/multicol_sim/blob/master/images/b_standardized_static.png?raw=true&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggsave(filename = &amp;quot;images/b_standardized_static.png&amp;quot;, width = 10, height = 7)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## R version 3.5.0 (2018-04-23)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 17134)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=German_Germany.1252  LC_CTYPE=German_Germany.1252   
## [3] LC_MONETARY=German_Germany.1252 LC_NUMERIC=C                   
## [5] LC_TIME=German_Germany.1252    
## 
## attached base packages:
## [1] grid      stats     graphics  grDevices utils     datasets  methods  
## [8] base     
## 
## other attached packages:
##  [1] bindrcpp_0.2.2     ggrepel_0.8.0      lm.beta_1.5-1     
##  [4] gridExtra_2.3      viridis_0.5.1      viridisLite_0.3.0 
##  [7] ecodist_2.0.1      forcats_0.3.0      stringr_1.3.0     
## [10] dplyr_0.7.5        readr_1.1.1        tidyr_0.8.1       
## [13] tibble_1.4.2       ggplot2_3.0.0.9000 tidyverse_1.2.1   
## [16] ggthemes_4.0.0     broom_0.4.4        purrr_0.2.4       
## [19] arm_1.10-1         lme4_1.1-17        Matrix_1.2-14     
## [22] MASS_7.3-49       
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_0.12.18     lubridate_1.7.4  lattice_0.20-35  assertthat_0.2.0
##  [5] rprojroot_1.3-2  digest_0.6.15    psych_1.8.3.3    R6_2.2.2        
##  [9] cellranger_1.1.0 plyr_1.8.4       backports_1.1.2  evaluate_0.10.1 
## [13] coda_0.19-1      httr_1.3.1       pillar_1.2.1     rlang_0.2.1     
## [17] lazyeval_0.2.1   readxl_1.1.0     minqa_1.2.4      rstudioapi_0.7  
## [21] nloptr_1.0.4     rmarkdown_1.9    labeling_0.3     splines_3.5.0   
## [25] foreign_0.8-70   munsell_0.4.3    compiler_3.5.0   modelr_0.1.1    
## [29] pkgconfig_2.0.1  mnormt_1.5-5     htmltools_0.3.6  tidyselect_0.2.4
## [33] crayon_1.3.4     withr_2.1.2      nlme_3.1-137     jsonlite_1.5    
## [37] gtable_0.2.0     pacman_0.4.6     magrittr_1.5     scales_0.5.0    
## [41] cli_1.0.0        stringi_1.1.7    reshape2_1.4.3   xml2_1.2.0      
## [45] tools_3.5.0      glue_1.3.0       hms_0.4.2        abind_1.4-5     
## [49] parallel_3.5.0   yaml_2.1.19      colorspace_1.4-0 rvest_0.3.2     
## [53] knitr_1.20       bindr_0.1.1      haven_1.1.2
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing Tweets of the ECPR General Conference 2018</title>
      <link>/post/ecprconf18/</link>
      <pubDate>Sat, 25 Aug 2018 00:00:00 +0200</pubDate>
      
      <guid>/post/ecprconf18/</guid>
      <description>

&lt;p&gt;This is a short notebook outlining the code used to scrape tweets
related to the ECPR Conference 2018 in Hamburg.&lt;/p&gt;

&lt;h2 id=&#34;packages&#34;&gt;Packages&lt;/h2&gt;

&lt;p&gt;Load the necessary packages&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# install pacman once if not avaible on your machine
# install.packages(&amp;quot;pacman&amp;quot;)

pacman::p_load(tidyverse, purrr, tidyr, rtweet, stringr, ggraph, igraph, tidygraph, forcats)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;get-data&#34;&gt;Get Data&lt;/h2&gt;

&lt;p&gt;If you want to scrape data yourself you have to
register a free account where you get your personal access point to
Twitter. Check out &lt;a href=&#34;https://github.com/mkearney/rtweet/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;rtweet&lt;/code&gt;&lt;/a&gt; on
github and follow their instructions for twitter authentication.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;twitter_token &amp;lt;- readRDS(&amp;quot;twitter_token.rds&amp;quot;)

rt &amp;lt;- search_tweets(
  &amp;quot;#ecprconf18 OR #ecprconf2018&amp;quot;, n = 5000, include_rts = F, retryonratelimit = T
)
save(rt, file = &amp;quot;data/rt.Rdata&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lets first look at the data structure and column names. Twitter returns
over 1,200 unique tweets.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;load(&amp;quot;data/rt.Rdata&amp;quot;)

rt %&amp;gt;% glimpse # the same as str, returns a df overview
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Observations: 1,399
## Variables: 88
## $ user_id                 &amp;lt;chr&amp;gt; &amp;quot;21951668&amp;quot;, &amp;quot;21951668&amp;quot;, &amp;quot;21951668&amp;quot;, &amp;quot;2...
## $ status_id               &amp;lt;chr&amp;gt; &amp;quot;1033324994552573952&amp;quot;, &amp;quot;10325642035535...
## $ created_at              &amp;lt;dttm&amp;gt; 2018-08-25 12:07:31, 2018-08-23 09:44...
## $ screen_name             &amp;lt;chr&amp;gt; &amp;quot;akreppel&amp;quot;, &amp;quot;akreppel&amp;quot;, &amp;quot;akreppel&amp;quot;, &amp;quot;a...
## $ text                    &amp;lt;chr&amp;gt; &amp;quot;While the croissants can be treachero...
## $ source                  &amp;lt;chr&amp;gt; &amp;quot;Twitter for iPhone&amp;quot;, &amp;quot;Twitter for iPa...
## $ display_text_width      &amp;lt;dbl&amp;gt; 92, 110, 37, 126, 215, 165, 158, 32, 2...
## $ reply_to_status_id      &amp;lt;chr&amp;gt; NA, &amp;quot;1032375076652347393&amp;quot;, NA, NA, NA,...
## $ reply_to_user_id        &amp;lt;chr&amp;gt; NA, &amp;quot;21951668&amp;quot;, NA, NA, NA, NA, NA, NA...
## $ reply_to_screen_name    &amp;lt;chr&amp;gt; NA, &amp;quot;akreppel&amp;quot;, NA, NA, NA, NA, NA, NA...
## $ is_quote                &amp;lt;lgl&amp;gt; FALSE, FALSE, FALSE, FALSE, FALSE, FAL...
## $ is_retweet              &amp;lt;lgl&amp;gt; FALSE, FALSE, FALSE, FALSE, FALSE, FAL...
## $ favorite_count          &amp;lt;int&amp;gt; 0, 2, 5, 2, 14, 9, 0, 2, 5, 5, 5, 2, 8...
## $ retweet_count           &amp;lt;int&amp;gt; 0, 0, 0, 0, 5, 5, 0, 0, 4, 1, 1, 2, 2,...
## $ hashtags                &amp;lt;list&amp;gt; [&amp;quot;ecprconf18&amp;quot;, &amp;quot;ECPRconf2018&amp;quot;, &amp;quot;ecprc...
## $ symbols                 &amp;lt;list&amp;gt; [NA, NA, NA, NA, NA, NA, NA, NA, NA, ...
## $ urls_url                &amp;lt;list&amp;gt; [NA, NA, NA, NA, &amp;quot;ecpr.eu/Events/Pane...
## $ urls_t.co               &amp;lt;list&amp;gt; [NA, NA, NA, NA, &amp;quot;https://t.co/KIzaT3...
## $ urls_expanded_url       &amp;lt;list&amp;gt; [NA, NA, NA, NA, &amp;quot;https://ecpr.eu/Eve...
## $ media_url               &amp;lt;list&amp;gt; [NA, NA, NA, NA, &amp;quot;http://pbs.twimg.co...
## $ media_t.co              &amp;lt;list&amp;gt; [NA, NA, NA, NA, &amp;quot;https://t.co/4Els2K...
## $ media_expanded_url      &amp;lt;list&amp;gt; [NA, NA, NA, NA, &amp;quot;https://twitter.com...
## $ media_type              &amp;lt;list&amp;gt; [NA, NA, NA, NA, &amp;quot;photo&amp;quot;, NA, &amp;quot;photo&amp;quot;...
## $ ext_media_url           &amp;lt;list&amp;gt; [NA, NA, NA, NA, &amp;quot;http://pbs.twimg.co...
## $ ext_media_t.co          &amp;lt;list&amp;gt; [NA, NA, NA, NA, &amp;quot;https://t.co/4Els2K...
## $ ext_media_expanded_url  &amp;lt;list&amp;gt; [NA, NA, NA, NA, &amp;quot;https://twitter.com...
## $ ext_media_type          &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...
## $ mentions_user_id        &amp;lt;list&amp;gt; [NA, NA, NA, NA, NA, &amp;quot;49383083&amp;quot;, NA, ...
## $ mentions_screen_name    &amp;lt;list&amp;gt; [NA, NA, NA, NA, NA, &amp;quot;ulrikeguerot&amp;quot;, ...
## $ lang                    &amp;lt;chr&amp;gt; &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;e...
## $ quoted_status_id        &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, &amp;quot;10329...
## $ quoted_text             &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, &amp;quot;\&amp;quot;Rev...
## $ quoted_created_at       &amp;lt;dttm&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, 2018-...
## $ quoted_source           &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, &amp;quot;Twitt...
## $ quoted_favorite_count   &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, 12, NA...
## $ quoted_retweet_count    &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, 3, NA,...
## $ quoted_user_id          &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, &amp;quot;18984...
## $ quoted_screen_name      &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, &amp;quot;Lafle...
## $ quoted_name             &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, &amp;quot;Jean-...
## $ quoted_followers_count  &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, 497, N...
## $ quoted_friends_count    &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, 356, N...
## $ quoted_statuses_count   &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, 335, N...
## $ quoted_location         &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, &amp;quot;Belgi...
## $ quoted_description      &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, &amp;quot;Assoc...
## $ quoted_verified         &amp;lt;lgl&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, FALSE,...
## $ retweet_status_id       &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...
## $ retweet_text            &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...
## $ retweet_created_at      &amp;lt;dttm&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N...
## $ retweet_source          &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...
## $ retweet_favorite_count  &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...
## $ retweet_retweet_count   &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...
## $ retweet_user_id         &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...
## $ retweet_screen_name     &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...
## $ retweet_name            &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...
## $ retweet_followers_count &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...
## $ retweet_friends_count   &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...
## $ retweet_statuses_count  &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...
## $ retweet_location        &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...
## $ retweet_description     &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...
## $ retweet_verified        &amp;lt;lgl&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...
## $ place_url               &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, &amp;quot;https...
## $ place_name              &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, &amp;quot;Hambu...
## $ place_full_name         &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, &amp;quot;Hambu...
## $ place_type              &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, &amp;quot;city&amp;quot;...
## $ country                 &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, &amp;quot;Germa...
## $ country_code            &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, &amp;quot;DE&amp;quot;, ...
## $ geo_coords              &amp;lt;list&amp;gt; [&amp;lt;NA, NA&amp;gt;, &amp;lt;NA, NA&amp;gt;, &amp;lt;NA, NA&amp;gt;, &amp;lt;NA, N...
## $ coords_coords           &amp;lt;list&amp;gt; [&amp;lt;NA, NA&amp;gt;, &amp;lt;NA, NA&amp;gt;, &amp;lt;NA, NA&amp;gt;, &amp;lt;NA, N...
## $ bbox_coords             &amp;lt;list&amp;gt; [&amp;lt;NA, NA, NA, NA, NA, NA, NA, NA&amp;gt;, &amp;lt;N...
## $ status_url              &amp;lt;chr&amp;gt; &amp;quot;https://twitter.com/akreppel/status/1...
## $ name                    &amp;lt;chr&amp;gt; &amp;quot;(((Amie Kreppel)))&amp;quot;, &amp;quot;(((Amie Kreppel...
## $ location                &amp;lt;chr&amp;gt; &amp;quot;Florida, USA&amp;quot;, &amp;quot;Florida, USA&amp;quot;, &amp;quot;Flori...
## $ description             &amp;lt;chr&amp;gt; &amp;quot;Jean Monnet Chair and Associate Profe...
## $ url                     &amp;lt;chr&amp;gt; NA, NA, NA, NA, &amp;quot;https://t.co/MslGy2Ml...
## $ protected               &amp;lt;lgl&amp;gt; FALSE, FALSE, FALSE, FALSE, FALSE, FAL...
## $ followers_count         &amp;lt;int&amp;gt; 296, 296, 296, 296, 1245, 1245, 1245, ...
## $ friends_count           &amp;lt;int&amp;gt; 355, 355, 355, 355, 475, 475, 475, 475...
## $ listed_count            &amp;lt;int&amp;gt; 12, 12, 12, 12, 85, 85, 85, 85, 109, 1...
## $ statuses_count          &amp;lt;int&amp;gt; 2789, 2789, 2789, 2789, 8805, 8805, 88...
## $ favourites_count        &amp;lt;int&amp;gt; 3980, 3980, 3980, 3980, 0, 0, 0, 0, 24...
## $ account_created_at      &amp;lt;dttm&amp;gt; 2009-02-26 01:23:21, 2009-02-26 01:23...
## $ verified                &amp;lt;lgl&amp;gt; FALSE, FALSE, FALSE, FALSE, FALSE, FAL...
## $ profile_url             &amp;lt;chr&amp;gt; NA, NA, NA, NA, &amp;quot;https://t.co/MslGy2Ml...
## $ profile_expanded_url    &amp;lt;chr&amp;gt; NA, NA, NA, NA, &amp;quot;http://www.jaruizso.e...
## $ account_lang            &amp;lt;chr&amp;gt; &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;e...
## $ profile_banner_url      &amp;lt;chr&amp;gt; &amp;quot;https://pbs.twimg.com/profile_banners...
## $ profile_background_url  &amp;lt;chr&amp;gt; &amp;quot;http://abs.twimg.com/images/themes/th...
## $ profile_image_url       &amp;lt;chr&amp;gt; &amp;quot;http://pbs.twimg.com/profile_images/2...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The top ten retweeted tweets.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# load(&amp;quot;rt.Rdata&amp;quot;)
rt %&amp;gt;% 
  select(screen_name, text, retweet_count) %&amp;gt;% 
  filter(!str_detect(text, &amp;quot;^RT&amp;quot;)) %&amp;gt;% 
  mutate(text = str_replace_all(text, &amp;quot;\\\n&amp;quot;, &amp;quot; &amp;quot;)) %&amp;gt;% 
  arrange(desc(retweet_count)) %&amp;gt;% 
  top_n(n = 10) %&amp;gt;% 
  knitr::kable(., format = &amp;quot;markdown&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;screen_name&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;text&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;retweet_count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;jrteruel&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cataluña y Escocia no son tan parecidas! Un análisis de discurso halla notables diferencias entre independentismos: recurrente retórica populista en Cat, pero no en Esc. Interesante esto d @josejolivas presentado en el panel de @_ignaciomolina &lt;a href=&#34;https://t.co/DuGF8ppfq6&#34; target=&#34;_blank&#34;&gt;https://t.co/DuGF8ppfq6&lt;/a&gt; #ecprconf18 &lt;a href=&#34;https://t.co/OAh4CMMlqc&#34; target=&#34;_blank&#34;&gt;https://t.co/OAh4CMMlqc&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;64&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;policy_politics&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A special issue challenging scholars of policy theory to change the way they produce and communicate research. Read it for free until 20 September: &lt;a href=&#34;https://t.co/iaMl4s08C6&#34; target=&#34;_blank&#34;&gt;https://t.co/iaMl4s08C6&lt;/a&gt; #ecprconf18 @ECPR @CairneyPaul @chris_weible &lt;a href=&#34;https://t.co/TsQBSlfN6T&#34; target=&#34;_blank&#34;&gt;https://t.co/TsQBSlfN6T&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;27&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;DelDemUCan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;The countdown begins for the #ecprconf18 in Hamburg! Section conveners @NicoleCurato &amp;amp; @_SofieMarien put together a schedule of democratic innovations panels here. Check it out &lt;a href=&#34;https://t.co/e8PVzo8foA&#34; target=&#34;_blank&#34;&gt;https://t.co/e8PVzo8foA&lt;/a&gt; &lt;a href=&#34;https://t.co/kfEwLE6swv&#34; target=&#34;_blank&#34;&gt;https://t.co/kfEwLE6swv&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;23&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;BJPolS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;We are pleased to offer free access to a selection of the most-cited BJPolS articles until the end of 2018: &lt;a href=&#34;https://t.co/G2wKHDnTdL&#34; target=&#34;_blank&#34;&gt;https://t.co/G2wKHDnTdL&lt;/a&gt; #ecprconf18 &lt;a href=&#34;https://t.co/rR7uPOwgCS&#34; target=&#34;_blank&#34;&gt;https://t.co/rR7uPOwgCS&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;antje_wiener&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Delighted to see the book published: #Contestation and #Constitution of #Norms in #GlobalIR ⁦ ⁦@CUP_PoliSci⁩ ⁦@CUP_Law⁩ ⁦@InternatlTheory⁩ ⁦@JacquiTrue⁩ @SassanGholiagha⁩ ⁦@JuttaBrunnee⁩ @womenalsoknow⁩ #ecprconf18 &lt;a href=&#34;https://t.co/IbZUclVRpd&#34; target=&#34;_blank&#34;&gt;https://t.co/IbZUclVRpd&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;CUP_PoliSci&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Do you want to know what to do with your first book proposal? Read a series of blog posts from a @CambridgeUP Publisher: &lt;a href=&#34;https://t.co/vGFEmb3r48&#34; target=&#34;_blank&#34;&gt;https://t.co/vGFEmb3r48&lt;/a&gt; #ecprconf18 &lt;a href=&#34;https://t.co/Z34hmGWfSe&#34; target=&#34;_blank&#34;&gt;https://t.co/Z34hmGWfSe&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ECPR&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Are you joining us at the University of Hamburg for #ecprconf18? Rejoice! The printed programme is now available to view online &lt;a href=&#34;https://t.co/GN8e81PaXx&#34; target=&#34;_blank&#34;&gt;https://t.co/GN8e81PaXx&lt;/a&gt; &lt;a href=&#34;https://t.co/4lZBbhLxV0&#34; target=&#34;_blank&#34;&gt;https://t.co/4lZBbhLxV0&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;LauraSeelkopf&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;And here is the detailed programme of our section on the politics of taxation #ecprconf18 : come and listen to 7 panels with 32 papers written by 48 scholars from 40 different institutions and four continents &lt;a href=&#34;https://t.co/1ZcM1No3ck&#34; target=&#34;_blank&#34;&gt;https://t.co/1ZcM1No3ck&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;sarahcpolitics&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Presenting at #ecprconf18 paper on political protest in the context of the global financial crisis. I find the crisis triggered mobilisations, but depressed participation trends more broadly. Saturday 2pm VMP 5, Rm 2067. @ValuesStudies. Download the paper: &lt;a href=&#34;https://t.co/TlPdTydQdf&#34; target=&#34;_blank&#34;&gt;https://t.co/TlPdTydQdf&lt;/a&gt; &lt;a href=&#34;https://t.co/PzzAPiPrYp&#34; target=&#34;_blank&#34;&gt;https://t.co/PzzAPiPrYp&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;kimgron&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;The #ecprconf18 starts on Thursday in Hamburg. We have a full #DemInno section 👍 Also, don’t miss the Standing Group on Democratic Innovations meeting on Friday at 1 pm, Building VMP 8, Room 105. Steering Committee candidates will present themselves there, election in September&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;timeline&#34;&gt;Timeline&lt;/h2&gt;

&lt;p&gt;What was the best time to tweet?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rt %&amp;gt;%
  ## parse date format
  mutate(
    cdate = created_at %&amp;gt;% 
      str_extract(&amp;quot;\\d{4}-\\d{2}-\\d{2}&amp;quot;) %&amp;gt;% 
      lubridate::ymd(),
    hour = lubridate::hour(created_at)
  ) %&amp;gt;%
  ## select relevant time period
  filter(cdate &amp;gt;= as.Date(&amp;quot;2018-08-23&amp;quot;)) %&amp;gt;% 
  ## count tweet per and and hour
  group_by(cdate, hour) %&amp;gt;%
  tally %&amp;gt;%
  ungroup %&amp;gt;%
  ggplot(aes(hour, n)) +
  geom_line() +
  ## split the visualization 
  facet_wrap(~cdate, ncol = 1) +
  theme_minimal() +
  ggtitle(&amp;quot;Number of Tweets by Day and Hour&amp;quot;) +
  xlab(&amp;quot;Hour of the Day&amp;quot;) +
  ylab(&amp;quot;Number of Tweets&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/favstats/ecprconf2018_twitter/blob/master/ecprconf2018_twitter_files/figure-gfm/unnamed-chunk-5-1.png?raw=true&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;h2 id=&#34;mentions-network&#34;&gt;Mentions Network&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rt_graph &amp;lt;- rt %&amp;gt;% 
  ## select relevant variables
  dplyr::select(screen_name, mentions_screen_name) %&amp;gt;% 
  ## unnest list of mentions_screen_name
  unnest %&amp;gt;% 
  ## count the number of coocurences
  group_by(screen_name, mentions_screen_name) %&amp;gt;% 
  tally(sort = T) %&amp;gt;%
  ungroup %&amp;gt;% 
  ## drop missing values
  drop_na %&amp;gt;% 
  ## filter those coocurences that appear at least 2 times
  filter(n &amp;gt; 1) %&amp;gt;% 
  ## transforming the dataframe to a graph object
  as_tbl_graph() %&amp;gt;% 
  ## calculating node centrality
  mutate(popularity = centrality_degree(mode = &#39;in&#39;))

rt_graph %&amp;gt;% 
  ## create graph layout
  ggraph(layout = &amp;quot;kk&amp;quot;) + 
  ## define edge aestetics
  geom_edge_fan(aes(alpha = n, edge_width = n, color = n)) + 
  ## scale down link saturation
  scale_edge_alpha(range = c(.5, .9)) +
  ## define note size param
  scale_edge_color_gradient(low = &amp;quot;gray50&amp;quot;, high = &amp;quot;#1874CD&amp;quot;) +
  geom_node_point(aes(size = popularity), color = &amp;quot;gray30&amp;quot;) +
  ## equal width and height
  coord_fixed() +
  ## plain theme
  theme_void() +
  ## title
  ggtitle(&amp;quot;#ecprconf18 Twitter Mentions Network&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/favstats/ecprconf2018_twitter/blob/master/ecprconf2018_twitter_files/figure-gfm/unnamed-chunk-6-1.png?raw=true&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rt_graph %&amp;gt;% 
  ## create graph layout
  ggraph(layout = &amp;quot;kk&amp;quot;) + 
  ## define edge aestetics
  geom_edge_fan(aes(alpha = n, edge_width = n, color = n)) + 
  ## scale down link saturation
  scale_edge_alpha(range = c(.5, .9)) +
  ## define note size param
  scale_edge_color_gradient(low = &amp;quot;gray50&amp;quot;, high = &amp;quot;#1874CD&amp;quot;) +
  geom_node_point(aes(size = popularity), color = &amp;quot;gray30&amp;quot;) +
  ## define node labels
  geom_node_text(aes(label = name), repel = T, fontface = &amp;quot;bold&amp;quot;) +
  ## equal width and height
  coord_fixed() +
  ## plain theme
  theme_void() +
  ## title
  ggtitle(&amp;quot;#ecprconf18 Twitter Mentions Network&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/favstats/ecprconf2018_twitter/blob/master/ecprconf2018_twitter_files/figure-gfm/unnamed-chunk-6-2.png?raw=true&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rt_graph %&amp;gt;% 
  ## create graph layout
  ggraph(layout = &amp;quot;circle&amp;quot;) + 
  ## define edge aestetics
  geom_edge_fan(aes(alpha = n, edge_width = n, color = n)) + 
  ## scale down link saturation
  scale_edge_alpha(range = c(.5, .9)) +
  ## define note size param
  scale_edge_color_gradient(low = &amp;quot;gray50&amp;quot;, high = &amp;quot;#1874CD&amp;quot;) +
  geom_node_point(aes(size = popularity), color = &amp;quot;gray30&amp;quot;) +
  ## define node labels
  geom_node_text(aes(label = name), repel = F, fontface = &amp;quot;bold&amp;quot;) +
  ## equal width and height
  coord_fixed() +
  ## plain theme
  theme_void() +
  ## title
  ggtitle(&amp;quot;#ecprconf18 Twitter Mentions Network&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/favstats/ecprconf2018_twitter/blob/master/ecprconf2018_twitter_files/figure-gfm/unnamed-chunk-6-3.png?raw=true&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;h3 id=&#34;smaller-mentions-network-n-2&#34;&gt;Smaller Mentions Network (n &amp;gt; 2)&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rt_graph2 &amp;lt;- rt %&amp;gt;% 
  ## select relevant variables
  dplyr::select(screen_name, mentions_screen_name) %&amp;gt;% 
  ## unnest list of mentions_screen_name
  unnest %&amp;gt;% 
  ## count the number of coocurences
  group_by(screen_name, mentions_screen_name) %&amp;gt;% 
  tally(sort = T) %&amp;gt;%
  ungroup %&amp;gt;% 
  ## drop missing values
  drop_na %&amp;gt;% 
  ## filter those coocurences that appear at least 2 times
  filter(n &amp;gt; 2) %&amp;gt;% 
  ## transforming the dataframe to a graph object
  as_tbl_graph() %&amp;gt;% 
  ## calculating node centrality
  mutate(popularity = centrality_degree(mode = &#39;in&#39;))

rt_graph2 %&amp;gt;% 
  ## create graph layout
  ggraph(layout = &amp;quot;kk&amp;quot;) + 
  ## define edge aestetics
  geom_edge_fan(aes(alpha = n, edge_width = n, color = n)) + 
  ## scale down link saturation
  scale_edge_alpha(range = c(.5, .9)) +
  ## define note size param
  scale_edge_color_gradient(low = &amp;quot;gray50&amp;quot;, high = &amp;quot;#1874CD&amp;quot;) +
  geom_node_point(aes(size = popularity), color = &amp;quot;gray30&amp;quot;) +
  ## equal width and height
  coord_fixed() +
  geom_node_text(aes(label = name), repel = T, fontface = &amp;quot;bold&amp;quot;) +
  ## plain theme
  theme_void() +
  ## title
  ggtitle(&amp;quot;#ecprconf18 Twitter Mentions Network&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/favstats/ecprconf2018_twitter/blob/master/ecprconf2018_twitter_files/figure-gfm/unnamed-chunk-7-1.png?raw=true&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;h2 id=&#34;most-frequent-hashtags&#34;&gt;Most Frequent Hashtags&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rt_hashtags &amp;lt;- rt %&amp;gt;% 
  select(hashtags) %&amp;gt;% 
  ## unnest list of hastags
  unnest %&amp;gt;% 
    na.omit %&amp;gt;% 
  ## clean hashtags
  mutate(hashtags = stringr::str_to_lower(hashtags) %&amp;gt;% 
           str_replace_all(&amp;quot;2018&amp;quot;, &amp;quot;18&amp;quot;) %&amp;gt;% 
           ## add #symbol to vector
           paste0(&amp;quot;#&amp;quot;, .)) %&amp;gt;% 
  ## count each hashtag and sort
  count(hashtags, sort = T) %&amp;gt;% 
  filter(n &amp;gt; 5)

rt_hashtags %&amp;gt;% 
  filter(hashtags != &amp;quot;#ecprconf18&amp;quot;) %&amp;gt;%
  mutate(hashtags = forcats::fct_reorder(hashtags, n)) %&amp;gt;% 
  ggplot(aes(hashtags, n)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;, alpha = .7) +
  coord_flip() +
  theme_minimal() +
  ggtitle(&amp;quot;Most Frequent Hastags related to #ecprconf18&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/favstats/ecprconf2018_twitter/blob/master/ecprconf2018_twitter_files/figure-gfm/unnamed-chunk-8-1.png?raw=true&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;h2 id=&#34;most-frequent-bigram-network&#34;&gt;Most Frequent Bigram Network&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gg_bigram &amp;lt;- rt %&amp;gt;%
  select(text) %&amp;gt;% 
  ## remove text noise
  mutate(text = stringr::str_remove_all(text, &amp;quot;w |amp &amp;quot;)) %&amp;gt;% 
  ## remove retweets
  filter(!stringr::str_detect(text, &amp;quot;^RT&amp;quot;)) %&amp;gt;% 
  ## remove urls
  mutate(text = stringr::str_remove_all(text, &amp;quot;https?[:]//[[:graph:]]+&amp;quot;)) %&amp;gt;% 
  mutate(id = 1:n()) %&amp;gt;% 
  ## split text into words
  tidytext::unnest_tokens(word, text, token = &amp;quot;words&amp;quot;) %&amp;gt;% 
  ## remove stop words
  anti_join(tidytext::stop_words) %&amp;gt;% 
  ## paste words to text by id
  group_by(id) %&amp;gt;% 
  summarise(text = paste(word, collapse = &amp;quot; &amp;quot;)) %&amp;gt;% 
  ungroup %&amp;gt;% 
  ## again split text into bigrams (word occurences or collocations)
  tidytext::unnest_tokens(bigram, text, token = &amp;quot;ngrams&amp;quot;, n = 2) %&amp;gt;% 
  separate(bigram, c(&amp;quot;word1&amp;quot;, &amp;quot;word2&amp;quot;), sep = &amp;quot; &amp;quot;) %&amp;gt;% 
  ## remove the hashtag and count bigrams 
  filter(word1 != &amp;quot;ecprconf18&amp;quot;, word2 != &amp;quot;ecprconf18&amp;quot;) %&amp;gt;%
  count(word1, word2, sort = T) %&amp;gt;% 
  ## select first 100
  slice(1:100) %&amp;gt;% 
  drop_na() %&amp;gt;%
  ## create tidy graph object
  as_tbl_graph() %&amp;gt;% 
  ## calculate node centrality
  mutate(Popularity = centrality_degree(mode = &#39;in&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gg_bigram %&amp;gt;% 
  ggraph() +
  geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
  geom_node_point(aes(size = Popularity)) + 
  geom_node_text(aes(label = name),  repel = TRUE) +
  theme_void() +
  scale_edge_alpha(&amp;quot;&amp;quot;, range = c(0.3, .6)) +
  ggtitle(&amp;quot;Top Bigram Network from Tweets using hashtag #ecprconf18&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/favstats/ecprconf2018_twitter/blob/master/ecprconf2018_twitter_files/figure-gfm/unnamed-chunk-10-1.png?raw=true&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## R version 3.5.0 (2018-04-23)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 17134)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=German_Germany.1252  LC_CTYPE=German_Germany.1252   
## [3] LC_MONETARY=German_Germany.1252 LC_NUMERIC=C                   
## [5] LC_TIME=German_Germany.1252    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] bindrcpp_0.2.2     tidygraph_1.1.0    igraph_1.2.1      
##  [4] ggraph_1.0.1.9999  rtweet_0.6.7.9000  forcats_0.3.0     
##  [7] stringr_1.3.0      dplyr_0.7.5        purrr_0.2.4       
## [10] readr_1.1.1        tidyr_0.8.1        tibble_1.4.2      
## [13] ggplot2_3.0.0.9000 tidyverse_1.2.1   
## 
## loaded via a namespace (and not attached):
##  [1] ggrepel_0.8.0     Rcpp_0.12.18      lubridate_1.7.4  
##  [4] lattice_0.20-35   assertthat_0.2.0  rprojroot_1.3-2  
##  [7] digest_0.6.15     psych_1.8.3.3     ggforce_0.1.3    
## [10] R6_2.2.2          cellranger_1.1.0  plyr_1.8.4       
## [13] backports_1.1.2   evaluate_0.10.1   highr_0.6        
## [16] httr_1.3.1        pillar_1.2.1      rlang_0.2.1      
## [19] lazyeval_0.2.1    readxl_1.1.0      rstudioapi_0.7   
## [22] Matrix_1.2-14     rmarkdown_1.9     labeling_0.3     
## [25] tidytext_0.1.9    foreign_0.8-70    munsell_0.4.3    
## [28] broom_0.4.4       janeaustenr_0.1.5 compiler_3.5.0   
## [31] modelr_0.1.1      pkgconfig_2.0.1   mnormt_1.5-5     
## [34] htmltools_0.3.6   tidyselect_0.2.4  gridExtra_2.3    
## [37] viridisLite_0.3.0 crayon_1.3.4      withr_2.1.2      
## [40] SnowballC_0.5.1   MASS_7.3-49       grid_3.5.0       
## [43] nlme_3.1-137      jsonlite_1.5      gtable_0.2.0     
## [46] pacman_0.4.6      magrittr_1.5      tokenizers_0.2.1 
## [49] units_0.6-0       scales_0.5.0      cli_1.0.0        
## [52] stringi_1.1.7     farver_1.0        reshape2_1.4.3   
## [55] viridis_0.5.1     xml2_1.2.0        tools_3.5.0      
## [58] glue_1.3.0        tweenr_0.1.5.9999 hms_0.4.2        
## [61] parallel_3.5.0    yaml_2.1.19       colorspace_1.4-0 
## [64] rvest_0.3.2       knitr_1.20        bindr_0.1.1      
## [67] haven_1.1.2
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Slides: Deliberation Across the World</title>
      <link>/project/deliberation/</link>
      <pubDate>Sat, 30 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/deliberation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Slides: Godly Governance</title>
      <link>/project/godly/</link>
      <pubDate>Sat, 30 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/godly/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Deliberation Across the World A Cross-National Examination of the Link Between Deliberation and Regime Legitimacy</title>
      <link>/publication/deliberation_paper/</link>
      <pubDate>Fri, 29 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/deliberation_paper/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Godly Governance - Support for Religious Governance in Arab Muslim-Majority Countries</title>
      <link>/publication/godlygovernance_paper/</link>
      <pubDate>Fri, 29 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/godlygovernance_paper/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Slides: Variants of Populism</title>
      <link>/project/populism/</link>
      <pubDate>Fri, 08 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/populism/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Slides: Decoding the Alt-Right</title>
      <link>/project/decoding/</link>
      <pubDate>Fri, 27 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/decoding/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Fighting Fake News and Post-Truth Politics with Behavioral Science: The Pro-Truth Pledge</title>
      <link>/publication/truthpledge_paper/</link>
      <pubDate>Mon, 12 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/truthpledge_paper/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Mapping Terror Attacks with Highcharter</title>
      <link>/post/terror_map/</link>
      <pubDate>Wed, 03 Jan 2018 00:00:00 +0100</pubDate>
      
      <guid>/post/terror_map/</guid>
      <description>&lt;p&gt;A few days ago someone asked me whether I could visualize the terror attacks of recent years in Europe. Here is an attempt to do exactly this.&lt;/p&gt;

&lt;p&gt;The code for this blogpost can be found in this &lt;a href=&#34;https://github.com/favstats/terror_map&#34; target=&#34;_blank&#34;&gt;GitHub Repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;First I need data about terror attacks. For this I downloaded the &lt;a href=&#34;https://www.start.umd.edu/gtd/&#34; target=&#34;_blank&#34;&gt;Global Terrorism Database&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now in Rstudio, I load in the needed packages:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pacman::p_load(tidyverse, magrittr, readxl, highcharter)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After I put the downloaded &lt;code&gt;.xlsx&lt;/code&gt; file into my working directory, I use &lt;code&gt;readxl&lt;/code&gt; to read in the data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;terror &amp;lt;- read_xlsx(&amp;quot;data/globalterrorism.xlsx&amp;quot;, sheet = &amp;quot;Data&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now comes the &lt;strong&gt;data wrangling part&lt;/strong&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;terror_map &amp;lt;- terror %&amp;gt;% 
  filter(iyear &amp;gt;= 2000) %&amp;gt;% 
  filter(region %in% c(8, 9)) %&amp;gt;% 
  filter(nkill &amp;gt; 0) %&amp;gt;% 
  filter(longitude &amp;lt;= 38) %&amp;gt;% 
  # filter(longitude &amp;gt;= 24) %&amp;gt;% 
  # filter(latitude &amp;gt;= 13) %&amp;gt;% 
  arrange(desc(nkill)) %&amp;gt;% 
  mutate(lon = longitude) %&amp;gt;% 
  mutate(lat = latitude) %&amp;gt;% 
  mutate(date = paste(iyear, imonth, iday, sep = &amp;quot;-&amp;quot;)) %&amp;gt;% 
  mutate(name = paste0(&amp;quot;&amp;lt;br&amp;gt; &amp;lt;b&amp;gt;Country:&amp;lt;/b&amp;gt; &amp;quot;, country_txt,
                       &amp;quot;&amp;lt;br&amp;gt; &amp;lt;b&amp;gt;City:&amp;lt;/b&amp;gt; &amp;quot;, city, 
                       &amp;quot;&amp;lt;br&amp;gt; &amp;lt;b&amp;gt;Date:&amp;lt;/b&amp;gt; &amp;quot;, date,
                       &amp;quot;&amp;lt;br&amp;gt; &amp;lt;b&amp;gt;Group:&amp;lt;/b&amp;gt; &amp;quot;, gname,
                       &amp;quot;&amp;lt;br&amp;gt; &amp;lt;b&amp;gt;Attack Type:&amp;lt;/b&amp;gt; &amp;quot;, attacktype1_txt,
                       &amp;quot;&amp;lt;br&amp;gt; &amp;lt;b&amp;gt;Target:&amp;lt;/b&amp;gt; &amp;quot;, target1,
                       &amp;quot;&amp;lt;br&amp;gt; &amp;lt;b&amp;gt;Death Count&amp;lt;/b&amp;gt;&amp;quot;)) %&amp;gt;% 
  select(name, lon = longitude, lat,  z = nkill)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And finally, I apply the &lt;code&gt;highcharter&lt;/code&gt; functions to create a beautiful interactive map.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;hcmap(&amp;quot;custom/europe&amp;quot;, showInLegend = FALSE) %&amp;gt;%
  hc_add_series(data = na.omit(terror_map), type = &amp;quot;mapbubble&amp;quot;,
                minSize = 0, maxSize = 30,
                name = &amp;quot;Terror Attack&amp;quot;, fillOpacity = 0.01) %&amp;gt;%
  highcharter::hc_title(text = &amp;quot;Terror Attacks with Casualties &amp;gt; 1 (2000 - 2016)&amp;quot;, 
                        style = list(fontWeight = &amp;quot;bold&amp;quot;)) %&amp;gt;%
  hc_subtitle(text = &amp;quot;Source: &amp;lt;a href=&#39;https://www.start.umd.edu/gtd/&#39;&amp;gt;Global Terrorism Database (GTD)&amp;lt;/a&amp;gt;&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The result:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/favstats/terror_map/blob/master/images/terror_map.png?raw=true&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Click &lt;a href=&#34;https://favstats.github.io/terror_map/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; for interactive version.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mapping your 2017 Geolocations: The Tidy Way</title>
      <link>/post/tidylocations/</link>
      <pubDate>Wed, 03 Jan 2018 00:00:00 +0100</pubDate>
      
      <guid>/post/tidylocations/</guid>
      <description>&lt;p&gt;A few days ago I stumbled upon an instagram user who created a cool map of his geolocations. You can get his code &lt;a href=&#34;https://github.com/asheshwor/R-maps/blob/master/05_ggmap_google_location.R&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. I had to reproduce these cool maps myself and why not also make a blogpost while doing so?&lt;/p&gt;

&lt;p&gt;The code for this blogpost can be found in this &lt;a href=&#34;https://github.com/favstats/tidy_locations&#34; target=&#34;_blank&#34;&gt;GitHub Repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;First I need my geolocation data. If you use Google and own an Android cell phone, you can download yours &lt;a href=&#34;https://takeout.google.com/settings/takeout&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now in Rstudio, I load in the needed packages:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pacman::p_load(ggmap, jsonlite, tidyverse, lubridate, magrittr)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After I put the downloaded &lt;code&gt;.json&lt;/code&gt; file into my working directory, I use &lt;code&gt;fromJSON&lt;/code&gt; to read in the data.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;loc_history &amp;lt;- fromJSON(&#39;data/Standortverlauf.json&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As a next step, we want to extract our locations from the data. The locations are located (ha!) under &lt;code&gt;loc_history$locations&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;locations &amp;lt;- loc_history$locations #we are only interested in locations
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now comes the &lt;strong&gt;data wrangling part&lt;/strong&gt;. We want the data to be in the right format. Especially the dates are easily transformed with the powerful &lt;code&gt;lubridate&lt;/code&gt; package. First we create a &lt;code&gt;time&lt;/code&gt; variable, using the &lt;code&gt;as_datetime&lt;/code&gt; function. Next we bring the longitudes and lattitudes in the right format, then we select three relevant variables &lt;code&gt;time&lt;/code&gt;, &lt;code&gt;lon&lt;/code&gt; and &lt;code&gt;lat&lt;/code&gt;. Lastly, we filter the dataframe so that we will only be left with data of 2017.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;locations %&amp;lt;&amp;gt;% 
 mutate(time = as_datetime(as.numeric(timestampMs) / 1000)) %&amp;gt;% 
 mutate(lon = longitudeE7/1E7) %&amp;gt;% 
 mutate(lat = latitudeE7/1E7) %&amp;gt;% 
 select(time, lon, lat) %&amp;gt;% 
 filter(time &amp;gt; as_datetime(&amp;quot;2017-01-01 00:00:01&amp;quot;) &amp;amp; time &amp;lt; as_datetime(&amp;quot;2017-12-31 23:59:59&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we come to the &lt;strong&gt;plotting part&lt;/strong&gt;! We type in our keyword and off we go with the &lt;code&gt;ggmap&lt;/code&gt; package!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;map1 &amp;lt;- &amp;quot;Stuttgart&amp;quot; %&amp;gt;% 
  qmap(maptype = &amp;quot;toner&amp;quot;, source = &amp;quot;stamen&amp;quot;, zoom = 14) + # creates the map
  geom_point(aes(x = lon, y = lat),                       # adding your data
             size = 1,
             data = locations,
             color = &amp;quot;red&amp;quot;, 
             alpha = 0.01)
map1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/favstats/tidy_locations/blob/master/images/map1.png?raw=true&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We could also zoom out and get a map from all of Germany&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;map2 &amp;lt;- &amp;quot;germany&amp;quot; %&amp;gt;% 
  qmap(maptype = &amp;quot;toner&amp;quot;, source = &amp;quot;stamen&amp;quot;, zoom = 5) + # creates the map
  geom_point(aes(x = lon, y = lat),                       # adding your data
             size = 1,
             data = locations,
             color = &amp;quot;red&amp;quot;, 
             alpha = 0.01)
map2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/favstats/tidy_locations/blob/master/images/map4.png?raw=true&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If we wanted to save the map on our computer, we could use the following code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggsave(map2, file = &amp;quot;images/map2.png&amp;quot;, height = 4, width = 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Note that the name of your &lt;code&gt;.json&lt;/code&gt; might differ from mine if you are not using Google in German.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>What do Arab Muslims think about ISIS (Daesh)?</title>
      <link>/post/daesh/</link>
      <pubDate>Tue, 26 Dec 2017 00:00:00 +0100</pubDate>
      
      <guid>/post/daesh/</guid>
      <description>

&lt;p&gt;Welcome to my &lt;em&gt;very first blogpost&lt;/em&gt;. Here I want to dive a little bit into how Arab Muslims see Daesh (&lt;em&gt;a term for ISIS often used by Arab speakers&lt;/em&gt;). The data I will be using is from the &lt;a href=&#34;http://www.arabbarometer.org/&#34; target=&#34;_blank&#34;&gt;Arab Barometer Wave 4&lt;/a&gt;, released just a few weeks ago. I will also be using the Arab Barometer in the future because it has a lot of interesting questions asked, including a small sample size of &lt;em&gt;Syrian refugees&lt;/em&gt;. So stay tuned.&lt;/p&gt;

&lt;p&gt;Survey data from the Middle East is especially interesting to me because it is a region that is often talked about without considering local voices.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: Whenever I speak of Arab Muslims in this blogpost, I am referring to the sample at hand.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Without further ado, here are some insights into the perception of Daesh in the Arab world.&lt;/p&gt;

&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-preparation&#34;&gt;Data Preparation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-threat-of-daesh&#34;&gt;The Threat of Daesh&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#threat-to-country&#34;&gt;Threat to Country&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#threat-to-region&#34;&gt;Threat to Region&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#does-daesh-represent-islam&#34;&gt;Does Daesh represent Islam?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#daesh-s-violence-and-goals&#34;&gt;Daesh&amp;rsquo;s violence and goals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#who-is-responsible-for-daesh&#34;&gt;Who is responsible for Daesh?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-next&#34;&gt;What is next?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/nav&gt;


&lt;h2 id=&#34;data-preparation&#34;&gt;Data Preparation&lt;/h2&gt;

&lt;p&gt;First of all, the data has to be prepared. For all of the following charts I use the provided weights that are meant to make the survey more representative.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Furthermore, the data is filtered by religious affiliation, only including Muslim respondents &lt;code&gt;q1012 == 1&lt;/code&gt;. Unfortunately, questions about Daesh were not asked in Egypt (which is included in the original survey project). This leaves us with &lt;em&gt;7149 respondents&lt;/em&gt; across 6 different countries including a sample of Syrian refugees.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;All questions that are of interest here, can be found in &lt;em&gt;Section VIII: Current Affairs&lt;/em&gt; of the &lt;a href=&#34;http://www.arabbarometer.org/sites/default/files/code_book/AB4%20English%20Codebook%20Final%20English_0.pdf&#34; target=&#34;_blank&#34;&gt;Arab Barometer Wave 4 Codebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you are interested in the code that produced the following charts, feel free to visit this &lt;a href=&#34;https://github.com/favstats/GodlyGovernance/&#34; target=&#34;_blank&#34;&gt;GitHub Repository&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;the-threat-of-daesh&#34;&gt;The Threat of Daesh&lt;/h2&gt;

&lt;p&gt;The first charts we are going to look at are visualizations of two questions:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Does Daesh pose a threat to our country? (&lt;code&gt;q826&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;Does Daesh pose a threat to the Arab region? (&lt;code&gt;q827&lt;/code&gt;)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Respondents could answer on a three-point scale:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Very grave threat&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Somewhat of a threat&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;No threat at all&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There were also two options for &lt;em&gt;missing values&lt;/em&gt; (not included in visualization):&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;I don’t know (Do not read)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Declined to answer (Do not read)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;Here come the charts:&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;threat-to-country&#34;&gt;Threat to Country&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/favstats/GodlyGovernance/blob/master/images/threat_cntry.png?raw=true&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Overall, one can see that pretty much all populations seem to be quite concerned about the threat that Daesh poses to their country. &lt;em&gt;Lebanon&lt;/em&gt; is on top of the chart with over &lt;strong&gt;98%&lt;/strong&gt; of people saying that Daesh poses somewhat of a threat or even a grave threat to their country. Not surprising, since the Lebanese army and the Hezbollah militia are &lt;a href=&#34;https://www.reuters.com/article/us-mideast-crisis-lebanon-syria/lebanese-army-hezbollah-announce-offensives-against-islamic-state-on-syrian-border-idUSKCN1AZ03G&#34; target=&#34;_blank&#34;&gt;directly involved&lt;/a&gt; in the Syrian civil war and the threat of terror attacks is always present.&lt;/p&gt;

&lt;p&gt;Next on the list is &lt;em&gt;Tunisia&lt;/em&gt;, where about &lt;strong&gt;95%&lt;/strong&gt; consider Daesh to be a threat to their country. This might have something to do with the unusual number of Tunisians who have gone to fight with Daesh and are expected to &lt;a href=&#34;https://www.nytimes.com/2017/02/25/world/europe/isis-tunisia.html&#34; target=&#34;_blank&#34;&gt;return soon to Tunisia&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The countries least concerned seem to be &lt;em&gt;Morocco&lt;/em&gt; and &lt;em&gt;Palestine&lt;/em&gt;, where only &lt;strong&gt;half the population&lt;/strong&gt; considers Daesh to be a grave threat, although it adds up to &lt;strong&gt;75%&lt;/strong&gt; who think they are at least somewhat of a threat.&lt;/p&gt;

&lt;h3 id=&#34;threat-to-region&#34;&gt;Threat to Region&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/favstats/GodlyGovernance/blob/master/images/threat_reg.png?raw=true&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Considering the entire Arab region, a clearer picture emerges. The fraction of people who don&amp;rsquo;t consider Daesh a threat at all is consistently &lt;strong&gt;below 10%&lt;/strong&gt;. This clearly shows that Arab Muslims are very concerned and well aware about the threat that Daesh poses to the entire region. Again, Lebanon and Tunisia appear on top of the chart, however, there don&amp;rsquo;t seem to be very significant differences between the countries.&lt;/p&gt;

&lt;h2 id=&#34;does-daesh-represent-islam&#34;&gt;Does Daesh represent Islam?&lt;/h2&gt;

&lt;p&gt;Next, we will take a look at a very interesting question. It is often alleged that Daesh represents what can be called &lt;em&gt;true Islam&lt;/em&gt;. As Graeme Wood put it in his widely cited &lt;em&gt;The Atlantic&lt;/em&gt; article &lt;a href=&#34;https://www.theatlantic.com/magazine/archive/2015/03/what-isis-really-wants/384980/9&#34; target=&#34;_blank&#34;&gt;What ISIS really wants&lt;/a&gt;: &amp;ldquo;The reality is that the Islamic State is Islamic. &lt;em&gt;Very Islamic.&lt;/em&gt;&amp;rdquo; And he isn&amp;rsquo;t alone with this perception. In fact, the organization itself claims to represent the only authentic form of Islam. But how do Arab Muslims themselves see this organization that claims to represent their faith? We are about to find out.&lt;/p&gt;

&lt;p&gt;We are looking at the following question:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;To what extent do you believe Daesh’s tactics are compatible with the teachings of Islam? (&lt;code&gt;q828&lt;/code&gt;)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Respondents were able to answer in the following way:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Certainly represents true Islam&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Represents true Islam&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Does not represent true Islam&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Certainly does not represent true Islam&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The categories for &lt;em&gt;missing values&lt;/em&gt; (don&amp;rsquo;t know and decline to answer) have been summarized into a single category.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: The chart includes differently worded answer categories, but only because they fit better on the graphic.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Here is the chart:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/favstats/GodlyGovernance/blob/master/images/compatible.png?raw=true&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Right from the get go, we can see that the left side of the chart is quite empty. The overwhelming majority of Arab Muslims does not believe that the tactics of Daesh are in any way compatible with Islam. However, it&amp;rsquo;s also worth investigating the fraction of people who do agree with this notion. Populations that most perceive the actions of Daesh as Islamic are found in Palestine (9.0%), Algeria (9.4%), Lebanon (5.6%) and Tunisia (5.3%).&lt;/p&gt;

&lt;h2 id=&#34;daesh-s-violence-and-goals&#34;&gt;Daesh&amp;rsquo;s violence and goals&lt;/h2&gt;

&lt;p&gt;The next chart we will be looking at goes in the same direction as the previous one. The questions asked are the following:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;To what extent do you agree with the goals of Daesh? (&lt;code&gt;q829&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;To what extent to do you support Daesh’s use of violence? (&lt;code&gt;q830&lt;/code&gt;)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Here is the chart:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/favstats/GodlyGovernance/blob/master/images/combined.png?raw=true&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Again, one can see that a very similar pattern emerges. The overwhelming majority of Arab Muslims does not agree with Daesh&amp;rsquo;s goals nor their use of violence. The highest approval ratings of both goals and use of violence can be found in the same countries that have the highest proportion of people that think Daesh&amp;rsquo;s tactics represent &lt;em&gt;true Islam&lt;/em&gt;: Palestine (6.4%; 5.4%), Algeria (4.9%; 5.1%), Lebanon (2.0%; 0%) and Tunisia (1.8%; 1.5%). Of course, one has to keep in mind that a sensitive question like this (asking flat out if someone supports use of violence) could be subject to &lt;em&gt;social desirability bias&lt;/em&gt;. However, the overall low number of declined answers indicates that the possible bias isn&amp;rsquo;t as severe as one might expect (although this can never be guaranteed).&lt;/p&gt;

&lt;h2 id=&#34;who-is-responsible-for-daesh&#34;&gt;Who is responsible for Daesh?&lt;/h2&gt;

&lt;p&gt;The last question I will be looking at is also very interesting. It asks about the origins of Daesh and who or what is responsible for its creation. Here is the phrasing:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Who or what do you think is responsible for creating Daesh? (&lt;code&gt;q830&lt;/code&gt; and &lt;code&gt;q831&lt;/code&gt;)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The variable (&lt;code&gt;q831&lt;/code&gt;) includes the &lt;em&gt;&amp;ldquo;Other&amp;rdquo;&lt;/em&gt; category. I opened it up and coded a few more categories. For example, I&amp;rsquo;ve summarized all Western countries mentioned into one category (however, &amp;ldquo;The United States&amp;rdquo; is the most mentioned Western country by far). I also created one category for all Arab countries that were mentioned.&lt;/p&gt;

&lt;p&gt;Here is the resulting chart:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/favstats/GodlyGovernance/blob/master/images/daesh_Arab Barometer.png?raw=true&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A quarter of people (25.36%) seem to blame the United States for the creation of Daesh. It is however not clear whether they mean that Western countries are literally responsible for Daesh&amp;rsquo;s creation (a quite popular conspiracy theory which strangely enough was also repeated by the &lt;a href=&#34;http://www.businessinsider.de/trump-obama-clinton-isis-2016-8?r=US&amp;amp;IR=T&#34; target=&#34;_blank&#34;&gt;current President of the United States during his bid for office&lt;/a&gt;) or whether they mean that the United States created the conditions that ultimately led to the creation of Daesh.&lt;/p&gt;

&lt;p&gt;However, this ambiguous interpretation is not readily available when it comes to blaming Israel for the creation of Daesh (14.02%) hinting at &lt;a href=&#34;https://www.adl.org/blog/anti-israel-conspiracy-theories-appear-in-arabic-language-media-in-wake-of-sinai-massacre&#34; target=&#34;_blank&#34;&gt;anti-Israeli and antisemitic conspiracy theories&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Another interesting number shows up when considering Iran (4.48%) which might be rooted in anti-Shia sentiments.&lt;/p&gt;

&lt;p&gt;Individual charts for each country can be found &lt;a href=&#34;https://imgur.com/a/fR4Li&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Addendum:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;After I was asked to include this: &lt;a href=&#34;https://imgur.com/a/wQV3H&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; are the same visualizations that only include &lt;em&gt;Sunni Muslims&lt;/em&gt;. Unfortunately, this question was only asked in Lebanon, Morocco, Algeria and Tunisia. However, the results are very similar to what is shown here.&lt;/p&gt;

&lt;h2 id=&#34;what-is-next&#34;&gt;What is next?&lt;/h2&gt;

&lt;p&gt;I am looking forward to further analyze the data.&lt;/p&gt;

&lt;p&gt;Here are some ideas:&lt;/p&gt;

&lt;p&gt;Extract the people who are sympathetic to Daesh (respondents who see them as representing Islam, agree with their violence and their goals) and find out how they compare to people who aren&amp;rsquo;t. For example, I could see how religious, educated, well-off or poor ISIS sympathizers are compared to their peers.&lt;/p&gt;

&lt;p&gt;Let me know if you have any other ideas, I am always open to suggestions!&lt;/p&gt;

&lt;p&gt;Here are some other projects I am currently working on and will appear in my blog in the near future:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The Evolution of the Atheist/Skeptic Community on YouTube&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Alt Right discourse on Mainstream media&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Who support populist parties in Europe (European Social Survey 8 Data)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Visualization of Terror Incidents in 2017&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Download the &lt;a href=&#34;http://www.arabbarometer.org/sites/default/files/code_book/Arab%20Barometer%20Fourth%20Wave%20Methodology.pdf&#34; target=&#34;_blank&#34;&gt;Arab Barometer IV Methodology Statement&lt;/a&gt; for more details. 
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Note that in a first version of this blog post the sample size was wrongly stated as 8259.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>LearnR - Modelldiagnostik</title>
      <link>/project/example-external-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/project/example-external-project/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
